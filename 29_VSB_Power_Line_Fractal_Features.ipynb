{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "1ZTBF18ZEONo"
   },
   "source": [
    "# Load data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "hidden": true,
    "id": "i26iu2geEeFH",
    "outputId": "1ef3a53d-7581-4777-c0d8-20ef3d65d80a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-784fe321-8586-4406-b244-1c2d92513fbe\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-784fe321-8586-4406-b244-1c2d92513fbe\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"suhasaithal\",\"key\":\"c87c41921fe9344b9c854878138972be\"}'}"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab\n",
    "# Run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "JomCl_wJEQz4",
    "outputId": "c10420a9-2fb6-410b-b96d-c578d69952bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 67 Nov 24 04:47 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CWgUnJ__E-Fj"
   },
   "outputs": [],
   "source": [
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "x3RosWP9FA0W"
   },
   "outputs": [],
   "source": [
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "J-6Mll4BGveY",
    "outputId": "a790c244-f393-4e26-f3e5-1a1322e50d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
      "Downloading sample_submission.csv to /content\n",
      "  0% 0.00/158k [00:00<?, ?B/s]\n",
      "100% 158k/158k [00:00<00:00, 60.6MB/s]\n",
      "Downloading test.parquet.zip to /content\n",
      "100% 6.97G/6.97G [04:14<00:00, 46.7MB/s]\n",
      "100% 6.97G/6.97G [04:14<00:00, 29.4MB/s]\n",
      "Downloading metadata_test.csv to /content\n",
      "  0% 0.00/257k [00:00<?, ?B/s]\n",
      "100% 257k/257k [00:00<00:00, 82.6MB/s]\n",
      "Downloading metadata_train.csv to /content\n",
      "  0% 0.00/115k [00:00<?, ?B/s]\n",
      "100% 115k/115k [00:00<00:00, 102MB/s]\n",
      "Downloading train.parquet.zip to /content\n",
      "100% 3.02G/3.02G [02:18<00:00, 18.5MB/s]\n",
      "100% 3.02G/3.02G [02:18<00:00, 23.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download data from Kaggle using its API\n",
    "!kaggle competitions download -c vsb-power-line-fault-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "pAkX0W2OIMKf"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49685924/extract-google-drive-zip-from-google-colab-notebook\n",
    "# !unzip /content/train.parquet.zip\n",
    "# !unzip /content/test.parquet.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "JRnqVf-QDw94"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "DT0232ZTQoWJ",
    "outputId": "831c48bd-2e96-430b-af1c-f457bfd882f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  libsnappy-dev\n",
      "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
      "Need to get 27.2 kB of archives.\n",
      "After this operation, 108 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsnappy-dev amd64 1.1.7-1 [27.2 kB]\n",
      "Fetched 27.2 kB in 1s (36.4 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package libsnappy-dev:amd64.\n",
      "(Reading database ... 144793 files and directories currently installed.)\n",
      "Preparing to unpack .../libsnappy-dev_1.1.7-1_amd64.deb ...\n",
      "Unpacking libsnappy-dev:amd64 (1.1.7-1) ...\n",
      "Setting up libsnappy-dev:amd64 (1.1.7-1) ...\n",
      "Collecting python-snappy\n",
      "  Downloading https://files.pythonhosted.org/packages/45/35/65d9f8cc537129894b4b32647d80212d1fa342877581c5b8a69872cea8be/python-snappy-0.5.4.tar.gz\n",
      "Building wheels for collected packages: python-snappy\n",
      "  Building wheel for python-snappy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-snappy: filename=python_snappy-0.5.4-cp36-cp36m-linux_x86_64.whl size=40379 sha256=f8cbcb0509831027e43bbd2e09cd62a0c314689c5bdd7dbb53c81fa02b7a45a9\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/aa/d5/459b834baa4c9f0ea918a38750fb29981e4a01ef523a936c06\n",
      "Successfully built python-snappy\n",
      "Installing collected packages: python-snappy\n",
      "Successfully installed python-snappy-0.5.4\n",
      "Collecting fastparquet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b9/844e32d0e3739e5695057dff3a3b9f4abc0fcccff466fdaadb8fedb0ee1d/fastparquet-0.4.1.tar.gz (28.6MB)\n",
      "\u001b[K     |████████████████████████████████| 28.6MB 158kB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.1.4)\n",
      "Requirement already satisfied: numba>=0.28 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (0.48.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.18.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastparquet) (20.4)\n",
      "Collecting thrift>=0.11.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->fastparquet) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->fastparquet) (2018.9)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.28->fastparquet) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.28->fastparquet) (50.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastparquet) (2.4.7)\n",
      "Building wheels for collected packages: fastparquet, thrift\n",
      "  Building wheel for fastparquet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fastparquet: filename=fastparquet-0.4.1-cp36-cp36m-linux_x86_64.whl size=7125484 sha256=e23880dc6f9702d1b872f68542800614e6ab02706c4ce63961c554376cd19eb4\n",
      "  Stored in directory: /root/.cache/pip/wheels/10/45/cf/492ccb908adde1dd2551bb509a56e4096cce9487167f525120\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for thrift: filename=thrift-0.13.0-cp36-cp36m-linux_x86_64.whl size=345246 sha256=3ea71c5457c106785a135ea2ffe6cd15b38ebe287d01d22a082df5e74539de69\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built fastparquet thrift\n",
      "Installing collected packages: thrift, fastparquet\n",
      "Successfully installed fastparquet-0.4.1 thrift-0.13.0\n",
      "Collecting kneed\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/6b/e130913aaaad1373060e259ab222ca2330672db696b297b082c3f3089fcc/kneed-0.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from kneed) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from kneed) (3.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from kneed) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->kneed) (1.15.0)\n",
      "Installing collected packages: kneed\n",
      "Successfully installed kneed-0.7.0\n",
      "Collecting git+https://github.com/raphaelvallat/entropy.git\n",
      "  Cloning https://github.com/raphaelvallat/entropy.git to /tmp/pip-req-build-o67n7szq\n",
      "  Running command git clone -q https://github.com/raphaelvallat/entropy.git /tmp/pip-req-build-o67n7szq\n",
      "Building wheels for collected packages: entropy\n",
      "  Building wheel for entropy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for entropy: filename=entropy-0.1.2-cp36-none-any.whl size=15611 sha256=6c5435eafa292104bab94997c2da303f1e198e2c0047194f7512e03252ee9478\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8dertdwx/wheels/60/ed/d3/b715e38438f1f39edb1383aea79c578073953b25fa576fc71e\n",
      "Successfully built entropy\n",
      "Installing collected packages: entropy\n",
      "Successfully installed entropy-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install libsnappy-dev\n",
    "!pip install python-snappy\n",
    "!pip install fastparquet\n",
    "!pip install kneed\n",
    "\n",
    "# https://github.com/raphaelvallat/entropy/issues/4#issue-533857629\n",
    "!pip install git+https://github.com/raphaelvallat/entropy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "LJ4pFzdNbFjX",
    "outputId": "30564786-cbf2-43f8-80ee-214fa51aacc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask as dd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "import gc\n",
    "import pywt\n",
    "from statsmodels.robust import mad\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import find_peaks, peak_widths, peak_prominences\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.signal import periodogram\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K # The backend gives us access to tensorflow operations and allow us to create attention class\n",
    "from keras import optimizers # Allows to access Adam class and modify some parameters\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoid overfitting\n",
    "from keras import activations\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# https://stackoverflow.com/a/56569206/4699076\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from kneed import KneeLocator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numba\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pylab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from entropy import *\n",
    "from prettytable import PrettyTable\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "lelJqaOsJ-rC",
    "outputId": "908ea742-9e90-4a6e-fbaa-96028dfc0ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import output\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xj7jm52WZjrL"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/Colab Notebooks/Assignment_29-VSB_Power_Line/'\n",
    "test_path = '/content/drive/My Drive/Colab Notebooks/Assignment_29-VSB_Power_Line/test_path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "XgcVEnO5J2Pd"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ROKhF2C1ID2d"
   },
   "outputs": [],
   "source": [
    "metadata_train = pd.read_csv(\"/content/metadata_train.csv\")\n",
    "metadata_test = pd.read_csv(\"/content/metadata_test.csv\")\n",
    "\n",
    "# train = pd.read_parquet('/content/train.parquet', engine='fastparquet')\n",
    "# test = pd.read_parquet('/content/test.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRQgQzM1w4-T"
   },
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYnDAggwulEs"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbL2bSPuO33R"
   },
   "source": [
    "**General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "opleV_hFhVDq"
   },
   "outputs": [],
   "source": [
    "def parallel_proc_func(func, cols, no_signals, init_offset, \n",
    "                       save_file_path='None', *args):\n",
    "  \"\"\"\n",
    "  Perform parallel processing of the given function. \n",
    "  If a function has to be parallelized over several items (signal_ids), then they\n",
    "  can be parallelized based on the number of cores present in the CPU.\n",
    "  The results are stored in a csv file.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  func : function object\n",
    "    The function which is to be run parallely.\n",
    "  cols : numpy.ndarray\n",
    "    Names of the result columns that is to be stored in the csv file\n",
    "  no_signals : int\n",
    "    Total number of signals to be processed\n",
    "  init_offset : int\n",
    "    Initial offset of the item (signal_id) from where the iterations are to begin\n",
    "  save_file_path : string (Default - 'None')\n",
    "    Path of the csv file where the results are to be stored.\n",
    "    If None, resulting dataframe is returned.\n",
    "  *args : Tuple\n",
    "    Additional parameters (along with signal id) that are needed for the func \n",
    "    object.  \n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None (Default)\n",
    "    If the file has to be saved, then the resultant data frame is stored in the\n",
    "    respective save_file_path. Hence, nothing is returned.\n",
    "  df : pandas.DataFrame\n",
    "    If the file is not to be saved, then the resultant data frame is returned.\n",
    "  \"\"\"\n",
    "  # Get number of cores in cpu: https://stackoverflow.com/a/1006337/4699076\n",
    "  batch_size = multiprocessing.cpu_count() # no. of cores in the cpu, needed for parallel processing  \n",
    "  end_val = no_signals//batch_size\n",
    "  # Save the file after processing (save_pt*batch_size) number of signals\n",
    "  save_pt = end_val//10\n",
    "  if save_pt == 0:\n",
    "    save_pt = 1\n",
    "\n",
    "  # Check if file is already processed\n",
    "  if os.path.isfile(save_file_path):\n",
    "    df = pd.read_csv(save_file_path)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    start_val = df.shape[0]//batch_size\n",
    "    print(f\"{df.shape[0]} signal ids already processed.\")\n",
    "    print(\"Processing the remaining signal ids...\")    \n",
    "  else:\n",
    "    # If specific columns are given, then create a dataframe with the given columns\n",
    "    if cols != 'None':\n",
    "      df = pd.DataFrame(columns=cols)\n",
    "    # Create an empty dataframe if no columns given\n",
    "    else:\n",
    "      df = pd.DataFrame()\n",
    "    if save_file_path != 'None':\n",
    "      df.to_csv(save_file_path)\n",
    "    \n",
    "    start_val = 0\n",
    "  \n",
    "  start_batch = datetime.now()\n",
    "  # Begin iteration\n",
    "  for i in range(start_val, end_val):\n",
    "    sig_id = np.arange(init_offset+(i*batch_size),\n",
    "                        init_offset+(i*batch_size)+batch_size)\n",
    "    # If additional arguments are provided for 'func' object, \n",
    "    # then create a list containing signal ids and new argument\n",
    "    if len(args) != 0:\n",
    "      func_args = [(x, args[0]) for x in sig_id]\n",
    "    # If new arguments are not provided, then create a list of signal ids\n",
    "    else:\n",
    "      func_args = sig_id\n",
    "    \n",
    "    # Begin parallel processing\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "      results = executor.map(func, func_args)\n",
    "      for result in results:\n",
    "        # For dataframe with pre-defined columns\n",
    "        if cols != 'None':\n",
    "          temp_df = pd.DataFrame(data=[result], columns=cols)\n",
    "        # For dataframes with no pre-defined columns\n",
    "        else:\n",
    "          temp_df = pd.DataFrame(data=[result])\n",
    "        df = df.append(temp_df, ignore_index=True)\n",
    "\n",
    "    # Save file at the checkpoint\n",
    "    if ((i+1)%save_pt==0):\n",
    "      # Calculate and print the number of signals processed\n",
    "      no_sigs_processed = (i+1)*batch_size\n",
    "      print(f\"{no_sigs_processed} ({np.round(((no_sigs_processed)/no_signals)*100, 2)}%) signals completed. Time taken: {datetime.now() - start_batch}\")\n",
    "      if save_file_path != 'None':\n",
    "        df.to_csv(save_file_path)\n",
    "      start_batch = datetime.now()\n",
    "\n",
    "  # If all the signals are already processed, then start_val will be equal to end_val. \n",
    "  if start_val != end_val:\n",
    "    no_signals_remaining = no_signals%batch_size\n",
    "    if no_signals_remaining!=0:\n",
    "      sig_id = np.arange(init_offset+(end_val*batch_size),\n",
    "                        init_offset+(end_val*batch_size)+no_signals_remaining)\n",
    "      if len(args) != 0:\n",
    "        func_args = [(x, args[0]) for x in sig_id]\n",
    "      else:\n",
    "        func_args = sig_id\n",
    "\n",
    "      # Parallel processing\n",
    "      with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        results = executor.map(func, func_args)\n",
    "        for result in results:\n",
    "          if cols != 'None':\n",
    "            temp_df = pd.DataFrame(data=[result], columns=cols)\n",
    "          else:\n",
    "            temp_df = pd.DataFrame(data=[result])\n",
    "          df = df.append(temp_df, ignore_index=True)\n",
    "\n",
    "  no_sigs_processed = no_signals\n",
    "  print(f\"{no_sigs_processed} ({np.round(((no_sigs_processed)/no_signals)*100, 2)}%) signals completed. Time taken: {datetime.now() - start_batch}\")\n",
    "  if save_file_path != 'None':\n",
    "    df.to_csv(save_file_path)\n",
    "  if save_file_path == 'None':\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huEjoP9dYNdM"
   },
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     49,
     90,
     139,
     184,
     218
    ],
    "id": "6XQlWZbWrBRU"
   },
   "outputs": [],
   "source": [
    "def get_knee(sorted_arr_len, vals_sorted, sensitivity, curve, direction):\n",
    "  # https://github.com/arvkevi/kneed\n",
    "  kneedle = KneeLocator(np.arange(len(vals_sorted)), vals_sorted, S=sensitivity, \n",
    "                        curve=curve, direction='increasing')\n",
    "  knee = kneedle.knee_y\n",
    "  knee_ind = kneedle.knee\n",
    "  return knee, knee_ind  \n",
    "\n",
    "def plot_knee(feat_name, vals_sorted, target_marker, knee, knee_ind, compare):\n",
    "  \"\"\"\n",
    "  Plot the knee plot of the signal.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  feat_name : string\n",
    "    Name of the feature which is to be plotted.\n",
    "    This is used for labelling the plot.\n",
    "  vals_sorted : numpy.ndarray\n",
    "    The x values of the plot.\n",
    "  target_marker : numpy.ndarray\n",
    "    Get the indices where the respective signal has target=1\n",
    "  knee : numpy.float64\n",
    "    The knee value of the series.\n",
    "  knee_ind : int\n",
    "    Index in the series where the knee value is obtained.\n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  if not compare:\n",
    "    plt.figure(figsize=(20,5))\n",
    "  x_vals = np.arange(len(vals_sorted))  \n",
    "  plt.plot(x_vals, vals_sorted, label='target=1', marker='s', markevery=target_marker)\n",
    "  plt.plot(x_vals, vals_sorted, label='target=0')\n",
    "  plt.vlines(knee_ind, plt.ylim()[0], plt.ylim()[1], linestyles='dashed', label='Knee Value ({})'.format(knee))\n",
    "  plt.legend()\n",
    "  plt.xlabel('Signal Number')\n",
    "  plt.ylabel(feat_name)\n",
    "  if compare:\n",
    "    plt.suptitle('{} Knee Plot - All Phases'.format(feat_name))\n",
    "  else:\n",
    "    plt.title('{} Knee Plot - All Phases'.format(feat_name))\n",
    "    plt.show()\n",
    "    print('='*100)\n",
    "\n",
    "def plot_vs_sigid(temp_metadata, feat_name, target_index, knee, compare):\n",
    "  \"\"\"\n",
    "  Plot the feature value of the signal with respect to its signal id.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  temp_metadata : pandas.DataFrame\n",
    "    Dataframe containing information of the features along with measurement id\n",
    "    and target information for that measurement id.\n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.\n",
    "  target_index : numpy.ndarray\n",
    "    Index value where that respective measurement id has target=1\n",
    "  knee : numpy.float64\n",
    "    Knee value for that feature.\n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value  \n",
    "  \"\"\"\n",
    "  x_vals = np.arange(temp_metadata.shape[0])\n",
    "  if not compare:\n",
    "    plt.figure(figsize=(25,10))\n",
    "  plt.plot(x_vals, temp_metadata[feat_name].values, label='target=0')\n",
    "  # https://stackoverflow.com/questions/21285885/remove-line-through-marker-in-matplotlib-legend\n",
    "  plt.plot(x_vals, temp_metadata[feat_name].values, marker='s', markerfacecolor='red', \n",
    "          markevery=list(target_index), label='target=1', linestyle='None')\n",
    "  plt.hlines(knee, plt.xlim()[0], plt.xlim()[1], linestyles='dashed', label='Knee Value ({})'.format(knee))\n",
    "  plt.legend()\n",
    "  plt.xlabel('Signal ID')\n",
    "  plt.ylabel(feat_name)\n",
    "  if compare:\n",
    "    plt.suptitle('{} Vs. Signal ID'.format(feat_name))    \n",
    "  else:    \n",
    "    plt.title('{} Vs. Signal ID'.format(feat_name))\n",
    "    plt.show()\n",
    "    print('='*100)    \n",
    "\n",
    "def plot_pdf(temp_metadata, feat_name, knee, compare):\n",
    "  \"\"\"\n",
    "  Plot the Probability Density Function for the given feature.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  temp_metadata : pandas.DataFrame\n",
    "    Dataframe containing information of the features along with measurement id\n",
    "    and target information for that measurement id.\n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.\n",
    "  knee : numpy.float64\n",
    "    Knee value for that feature.\n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  target = temp_metadata[temp_metadata['target']==1][feat_name].values\n",
    "  no_target = temp_metadata[temp_metadata['target']==0][feat_name].values\n",
    "\n",
    "  if not compare:\n",
    "    print(\"Mean of all {} values: {}\".format(feat_name, np.mean(temp_metadata[feat_name].values)))\n",
    "    print(\"Mean of {} with target=1: {}\".format(feat_name, np.mean(target)))\n",
    "    print(\"Mean of {} with target=0: {}\".format(feat_name, np.mean(no_target)))  \n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "  ax1 = sns.distplot(temp_metadata[feat_name].values, kde=True, hist=False, label='combined')\n",
    "  ax2 = sns.distplot(target, kde=True, hist=False, label='target=1')\n",
    "  ax3 = sns.distplot(no_target, kde=True, hist=False, label='target=0', )\n",
    "\n",
    "  ax1.axvline(np.mean(temp_metadata[feat_name].values), color='blue')\n",
    "  ax2.axvline(np.mean(target), color='green')\n",
    "  ax3.axvline(np.mean(no_target), color='red')\n",
    "\n",
    "  plt.vlines(knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed', label='Knee Value ({})'.format(knee))\n",
    "  plt.legend()\n",
    "  plt.xlabel(feat_name)\n",
    "  plt.ylabel('PDF')\n",
    "  if compare:\n",
    "    plt.suptitle('PDF of {}'.format(feat_name))\n",
    "  else:\n",
    "    plt.title('PDF of {}'.format(feat_name))\n",
    "    plt.show()\n",
    "    print('='*100)\n",
    "\n",
    "def plot_cdf(feat_name, vals_sorted, knee, compare):\n",
    "  \"\"\"\n",
    "  Plot the Cumulative Density Function of the given feature.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.\n",
    "  vals_sorted : numpy.ndarray\n",
    "    x values of the plot.\n",
    "  knee : numpy.float64\n",
    "    Knee value for that feature.\n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  knee_perc = (((np.where(vals_sorted==knee)[0][0])/len(vals_sorted))*100)\n",
    "  percentile_vals = []\n",
    "  perc = 0\n",
    "  for ind, val in enumerate(vals_sorted):\n",
    "    if ind==int(perc*len(vals_sorted)):\n",
    "      percentile_vals.append(val)      \n",
    "      perc += 0.01\n",
    "  percentile_vals.append(vals_sorted[-1])\n",
    "  y_vals = np.linspace(0, 1, 101)\n",
    "\n",
    "  if not compare:\n",
    "    plt.figure(figsize=(15,5))\n",
    "  plt.plot(percentile_vals, y_vals)\n",
    "  plt.vlines(knee, plt.ylim()[0], plt.ylim()[1], linestyle='dashed', label='Knee Value ({})'.format(knee))\n",
    "  plt.hlines((knee_perc/100), plt.xlim()[0], plt.xlim()[1], linestyles='dashdot', label='Knee Value Percentile ({}%)'.format(np.round(knee_perc,2)))\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.xlabel(feat_name)\n",
    "  plt.ylabel('CDF')\n",
    "  if compare:\n",
    "    plt.suptitle('CDF of {}'.format(feat_name))\n",
    "  else:\n",
    "    plt.title('CDF of {}'.format(feat_name))\n",
    "    plt.show()\n",
    "    print('='*100)\n",
    "\n",
    "def plot_box(temp_metadata, feat_name, vals_sorted, knee, compare):\n",
    "  \"\"\"\n",
    "  Plot the box plot for the given feature.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  temp_metadata : pandas.DataFrame\n",
    "    Dataframe containing information of the features along with measurement id\n",
    "    and target information for that measurement id.  \n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.\n",
    "  vals_sorted : numpy.ndarray\n",
    "    x values of the plot.\n",
    "  knee : numpy.float64\n",
    "    Knee value for that feature.\n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  knee_perc = (((np.where(vals_sorted==knee)[0][0])/len(vals_sorted))*100)\n",
    "  sns.boxplot(x=temp_metadata['target'], y=temp_metadata[feat_name])\n",
    "  plt.hlines(knee, plt.xlim()[0], plt.xlim()[1], linestyles='dashed', label='Knee Value Percentile ({}%)'.format(np.round(knee_perc,2)))\n",
    "  plt.legend()\n",
    "  if compare:\n",
    "    plt.suptitle('Box plot of signals with target=0 and target=1')\n",
    "  else:\n",
    "    plt.title('Box plot of signals with target=0 and target=1')\n",
    "    plt.show()  \n",
    "    print('='*100)\n",
    "\n",
    "def logistic_plot(temp_metadata, feat_name, compare):\n",
    "  \"\"\"\n",
    "  Plot logistic plot for the given feature.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  temp_metadata : pandas.DataFrame\n",
    "    Dataframe containing information of the features along with measurement id\n",
    "    and target information for that measurement id.  \n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.  \n",
    "  compare : boolean\n",
    "    If True the plot is displayed in the subplot. \n",
    "    If False then the entire space is used for the plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "\n",
    "  References\n",
    "  ----------\n",
    "  .. [1] https://www.kaggle.com/mark4h/vsb-1st-place-solution\n",
    "  \"\"\"\n",
    "  sns.set_context(\"paper\", font_scale=2)\n",
    "  # fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "  sns.regplot(feat_name, 'target', temp_metadata, logistic=True, \n",
    "              n_boot=100, y_jitter=.1, \n",
    "              scatter_kws={'alpha':0.1, 'edgecolor':'none'})\n",
    "  if compare:\n",
    "    plt.suptitle('Logistic plot of signals with target=0 and target=1')\n",
    "  else:\n",
    "    plt.title('Logistic plot of signals with target=0 and target=1')\n",
    "    plt.show()\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     121,
     168
    ],
    "id": "2aCQ7pBtqViG"
   },
   "outputs": [],
   "source": [
    "def feature_eda(temp_metadata, feat_name, plot_name, phase, compare, \n",
    "                sensitivity=1, curve='convex'):\n",
    "  \"\"\"\n",
    "  Performs Exploratory Data Analysis for the given feature.\n",
    "\n",
    "  Following information is displayed upon calling this function:\n",
    "  * Knee plot\n",
    "  * Feature value vs. signal id\n",
    "  * PDF\n",
    "  * CDF\n",
    "  * Box plot\n",
    "  * Logistic plot\n",
    "  * Statistics\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  temp_metadata : pandas.DataFrame\n",
    "    Dataframe containing information of the features along with measurement id\n",
    "    and target information for that measurement id.  \n",
    "  feat_name : string\n",
    "    Name of the feature that is to be plotted.\n",
    "  plot_name : string\n",
    "    Name of the plot that is to be displayed.\n",
    "    Following are the valid strings for the plot:\n",
    "    * 'knee' - to display only knee plot\n",
    "    * 'sigid' - to display only feature vs. signal id plot\n",
    "    * 'pdf' - to display only the pdf plot\n",
    "    * 'cdf' - to display only the cdf plot\n",
    "    * 'box' - to display only the box plot\n",
    "    * 'stats' - to display only statisticss\n",
    "    * 'all' - to display all the plots and statistics\n",
    "  phase : string\n",
    "    This is to process the features with respect to signal id or with respect\n",
    "    to measurement id.\n",
    "    Following are valid strings:\n",
    "    * 'one_phase' - when feature is calculated for each signal id\n",
    "    * 'multi_phase' - when 3 phases of the signal is combined and processed\n",
    "  compare : boolean\n",
    "    If True plots are displayed in a subplot for comparing the two plots.\n",
    "    If False then the entire space is used to display the plot.\n",
    "  sensitivity : float (Default=1)\n",
    "    This value adjusts the position where the knee value is to be detected.\n",
    "  curve : string (Default='convex')\n",
    "    How the values sorted in the ascending order look like.\n",
    "    If 'convex' - sorted values look like exp(x) curve\n",
    "    If 'concave' - sorted values look like sigmoid(x) curve\n",
    "    \n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  # Get the indices where the target is 1\n",
    "  if phase=='one_phase':\n",
    "    target_index = temp_metadata[temp_metadata['target']==1]['signal_id'].values\n",
    "  elif phase=='multi_phase':\n",
    "    target_index = temp_metadata[temp_metadata['target']==1]['id_measurement'].values\n",
    "\n",
    "  # Sort the feature values in ascending order\n",
    "  vals_sorted = np.sort(temp_metadata[feat_name].values)\n",
    "\n",
    "  # Get the original indices of the sorted feature values\n",
    "  vals_ind = np.argsort(temp_metadata[feat_name].values)\n",
    "\n",
    "  # Get the original indices of the sorted feature values where target is 1\n",
    "  target_marker = []\n",
    "  for ind, val in enumerate(target_index):\n",
    "    target_marker.append(np.where(vals_ind==val)[0][0])\n",
    "\n",
    "  # https://github.com/arvkevi/kneed\n",
    "  kneedle = KneeLocator(np.arange(len(vals_sorted)), vals_sorted, S=sensitivity, \n",
    "                        curve=curve, direction='increasing')\n",
    "  knee = kneedle.knee_y\n",
    "  knee_ind = kneedle.knee\n",
    "\n",
    "  # If plot_name = 'all', then plot all the graphs\n",
    "  # Else plot the respective graphs\n",
    "  # Plot knee plot\n",
    "  if (plot_name=='all') | (plot_name=='knee'):\n",
    "    plot_knee(feat_name, vals_sorted, target_marker, knee, knee_ind, compare)\n",
    "  \n",
    "  if (plot_name=='all') & (not compare):\n",
    "    print(\"The knee value of the {} is: {}\".format(feat_name, knee))\n",
    "    print(f\"Number of signals with {feat_name} greater than {knee} and target=1: {len(np.where(np.array(target_marker) > knee_ind)[0])}({np.round((len(np.where(np.array(target_marker) > knee_ind)[0]) / temp_metadata.shape[0])*100 , 2)}%)\")\n",
    "    print(f\"Number of signals with {feat_name} greater than {knee} and target=0: {len(np.where(vals_sorted > knee)[0])}({np.round((len(np.where(vals_sorted > knee)[0]) / temp_metadata.shape[0])*100 , 2)}%)\")\n",
    "    print(\"Number of signals with target=1: {}\".format(len(target_index)))\n",
    "\n",
    "  # Plot feature values vs signal id\n",
    "  if (plot_name=='all') | (plot_name=='sigid'):\n",
    "    plot_vs_sigid(temp_metadata, feat_name, target_index, knee, compare)\n",
    "\n",
    "  # Plot PDF\n",
    "  if (plot_name=='all') | (plot_name=='pdf'):\n",
    "    plot_pdf(temp_metadata, feat_name, knee, compare)\n",
    "\n",
    "  # Plot CDF\n",
    "  if (plot_name=='all') | (plot_name=='cdf'):\n",
    "    plot_cdf(feat_name, vals_sorted, knee, compare)\n",
    "\n",
    "  # Plot box plot\n",
    "  if (plot_name=='all') | (plot_name=='box'):\n",
    "    plot_box(temp_metadata, feat_name, vals_sorted, knee, compare)\n",
    "  \n",
    "  # Plot logistic plot\n",
    "  if (plot_name=='all') | (plot_name=='logistic'):\n",
    "    logistic_plot(temp_metadata, feat_name, compare)\n",
    "  \n",
    "  # Print statistics\n",
    "  if ((plot_name=='all') | (plot_name=='stats')) & (not compare):\n",
    "    # https://stackoverflow.com/questions/34816769/getting-values-in-seaborn-boxplot\n",
    "    stats = temp_metadata[feat_name].describe()\n",
    "    print(f\"Statistics of all signals:\\n{stats}\")    \n",
    "    print(f\"Inter Quartile Range of all Signals: {stats[6]-stats[4]}\\n\")\n",
    "\n",
    "    stats = temp_metadata.groupby('target')[feat_name].describe().iloc[0]    \n",
    "    print(f\"Statistics of signals with target=0:\\n{stats}\")\n",
    "    print(f\"Inter Quartile Range of all signals with target=0: {stats[6]-stats[4]}\\n\")\n",
    "\n",
    "    stats = temp_metadata.groupby('target')[feat_name].describe().iloc[1]\n",
    "    print(f\"Statistics of signals with target=1:\\n{stats}\")\n",
    "    print(f\"Inter Quartile Range of all signals with target=1: {stats[6]-stats[4]}\\n\")\n",
    "\n",
    "def pca_plots(threshold_pos, threshold_neg, all_feats_df, cols, labels):\n",
    "  \"\"\"\n",
    "  Plots PCA plot for the given data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  threshold_pos : float\n",
    "    Positive threshold value for the mean correlation value for each feature.\n",
    "    It will select only those features whose mean correlation value is less than\n",
    "    this value.\n",
    "  threshold_neg : float\n",
    "    Negative threshold value for the mean correlation value for each feature.\n",
    "    It will select only those features whose mean correlation value is greater \n",
    "    than this value.\n",
    "  all_feats_df : pandas.DataFrame\n",
    "    Dataframe containing all the feature values. \n",
    "    These feature values are not standardized.\n",
    "  cols : list\n",
    "    List of names of all the features.    \n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pca_df : pandas.DataFrame\n",
    "    Dataframe comprising the reduced dimension.\n",
    "  pca_cols : list\n",
    "    List of strings specifying the selected columns for the given thresholds.\n",
    "  pca.explained_variance_ : numpy.array\n",
    "    Explained variance values (lambda) corresponding to each principal\n",
    "    component.\n",
    "  \"\"\"  \n",
    "  pca_cols = []\n",
    "  # Select the features whose mean correlation value lies between positive and \n",
    "  # negative threshold values\n",
    "  for ind, col in enumerate(cols):\n",
    "    if mean_corr[ind] < threshold_pos and mean_corr[ind] > threshold_neg:\n",
    "      pca_cols.append(col)\n",
    "  # Standardize the features\n",
    "  std = StandardScaler()\n",
    "  s = std.fit_transform(all_feats_df[pca_cols])\n",
    "\n",
    "  # Get all the principal components\n",
    "  pca = PCA()\n",
    "  pca_s = pca.fit_transform(s)\n",
    "  pca_df = pd.DataFrame(pca_s[:,0:2], columns=['1st_principal', '2nd_principal'])\n",
    "  pca_df['label'] = labels  \n",
    "  return pca_df, pca_cols, pca.explained_variance_\n",
    "\n",
    "def tsne_plots(threshold_pos, threshold_neg, all_feats_df, cols, labels, perp, iter):\n",
    "  \"\"\"\n",
    "  Plots TSNE plot for the given data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  threshold_pos : float\n",
    "    Positive threshold value for the mean correlation value for each feature.\n",
    "    It will select only those features whose mean correlation value is less than\n",
    "    this value.\n",
    "  threshold_neg : float\n",
    "    Negative threshold value for the mean correlation value for each feature.\n",
    "    It will select only those features whose mean correlation value is greater \n",
    "    than this value.\n",
    "  all_feats_df : pandas.DataFrame\n",
    "    Dataframe containing all the feature values. \n",
    "    These feature values are not standardized.\n",
    "  cols : list\n",
    "    List of names of all the features.        \n",
    "  perp : int\n",
    "    Specify the required perplexity in the TSNE plot.\n",
    "  iter : int\n",
    "    Specify the number of iterations to run for the TSNE plot.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tsne_df : pandas.DataFrame\n",
    "    Dataframe comprising the reduced dimension.    \n",
    "  \"\"\"\n",
    "  tsne_cols = []\n",
    "  # Select the features whose mean correlation value lies between positive and \n",
    "  # negative threshold values\n",
    "  for ind, col in enumerate(cols):\n",
    "    if mean_corr[ind] < threshold_pos and mean_corr[ind] > threshold_neg:\n",
    "      tsne_cols.append(col)\n",
    "  # Standardize the features\n",
    "  std = StandardScaler()\n",
    "  s = std.fit_transform(all_feats_df[tsne_cols])\n",
    "  tsne = TSNE(n_components=2, perplexity=perp, n_iter=iter, random_state=42)\n",
    "  tsne_s = tsne.fit_transform(s)\n",
    "  tsne_df = pd.DataFrame(tsne_s, columns=['Dim_1','Dim_2'])\n",
    "  tsne_df['label'] = labels\n",
    "  return tsne_df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     43,
     80
    ],
    "id": "Dw7iALYL_cdS"
   },
   "outputs": [],
   "source": [
    "plt_list = ['knee', 'sigid', 'pdf', 'cdf', 'box', 'logistic']\n",
    "title_names = ['Original', 'Outlier processed']\n",
    "\n",
    "def plot_compare(feat_name, phase, plt_list=plt_list, title_names=title_names, *args):\n",
    "  \"\"\"\n",
    "  Display the plots side by side for comparison.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  feat_name : string\n",
    "    Name of the feature to be compared in the Dataframe 1.\n",
    "  plt_list : list\n",
    "    Names of the plots to be displayed\n",
    "  original_df : pandas.DataFrame\n",
    "    Dataframe 1 to be compared.\n",
    "  denoised_df : pandas.DataFrame\n",
    "    Dataframe 2 to be compared.\n",
    "  phase : string\n",
    "    This is to process the features with respect to signal id or with respect\n",
    "    to measurement id.\n",
    "    Following are valid strings:\n",
    "    * 'one_phase' - when feature is calculated for each signal id\n",
    "    * 'multi_phase' - when 3 phases of the signal is combined and processed\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"\n",
    "  no_plts = len(plt_list)\n",
    "  no_compares = len(args[0])  \n",
    "  for ind, plt_name in enumerate(plt_list):    \n",
    "    for args_val in args:\n",
    "      plt.figure(figsize=(100,25))\n",
    "      count = 0\n",
    "      for df_ind, df in enumerate(args_val):\n",
    "        count += 1               \n",
    "        # plt.subplot(no_plts,no_compares,count)\n",
    "        plt.subplot(1,no_compares,count)\n",
    "        feature_eda(df, feat_name, plt_name, phase, True, 1)\n",
    "        plt.title(title_names[df_ind])\n",
    "      plt.show()\n",
    "      print('='*150)\n",
    "\n",
    "def plot_multi_compare(feat_list, phase, plt_list, df):\n",
    "  \"\"\"\n",
    "  Display multiple plots side by side for comparison. \n",
    "  Here the comparison is among different features of the single dataframe.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  feat_list : list\n",
    "    List of names of the features to be compared in the Dataframe df.\n",
    "  phase : string\n",
    "    This is to process the features with respect to signal id or with respect\n",
    "    to measurement id.\n",
    "    Following are valid strings:\n",
    "    * 'one_phase' - when feature is calculated for each signal id\n",
    "    * 'multi_phase' - when 3 phases of the signal is combined and processed    \n",
    "  plt_list : list\n",
    "    Names of the plots to be displayed\n",
    "  df : pandas.DataFrame\n",
    "    Dataframe containing the features to be compared.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"  \n",
    "  no_plts = len(plt_list)\n",
    "  no_compares = len(feat_list)    \n",
    "  for ind, plt_name in enumerate(plt_list):\n",
    "    plt.figure(figsize=(100,25))\n",
    "    count = 0\n",
    "    for feat_ind, feat_name in enumerate(feat_list):\n",
    "      count += 1\n",
    "      plt.subplot(1, no_compares, count)\n",
    "      feature_eda(df, feat_name, plt_name, phase, True)\n",
    "      plt.title(feat_name)\n",
    "    plt.show()\n",
    "    print('='*150)\n",
    "\n",
    "def plot_multi_compare_1(feat_list, phase, plt_list, df):\n",
    "  \"\"\"\n",
    "  Display multiple plots side by side for comparison.\n",
    "  Here the comparison is among different features of the single dataframe. \n",
    "  This differs from plot_multi_compare in the number of plot that are compared.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  feat_list : list\n",
    "    List of names of the features to be compared in the Dataframe df.\n",
    "  phase : string\n",
    "    This is to process the features with respect to signal id or with respect\n",
    "    to measurement id.\n",
    "    Following are valid strings:\n",
    "    * 'one_phase' - when feature is calculated for each signal id\n",
    "    * 'multi_phase' - when 3 phases of the signal is combined and processed    \n",
    "  plt_list : list\n",
    "    Names of the plots to be displayed\n",
    "  df : pandas.DataFrame\n",
    "    Dataframe containing the features to be compared.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None : Function does not return any value\n",
    "  \"\"\"    \n",
    "  no_plts = len(plt_list)\n",
    "  no_cols = 3\n",
    "  no_rows = np.ceil(len(feat_list)/no_cols)\n",
    "  \n",
    "  for ind, plt_name in enumerate(plt_list):\n",
    "    # plt.figure(figsize=(no_rows*row_size,no_cols*col_size))\n",
    "    plt.figure(figsize=(50,50))\n",
    "    count = 0\n",
    "    for feat_ind, feat_name in enumerate(feat_list):\n",
    "      count += 1\n",
    "      plt.subplot(no_rows, no_cols, count)\n",
    "      feature_eda(df, feat_name, plt_name, phase, True)\n",
    "      plt.title(feat_name)\n",
    "    plt.show()\n",
    "    print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOiTvIZqYGX1"
   },
   "source": [
    "**Discrete Wavelet Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "SAgH1ftvPw-e"
   },
   "outputs": [],
   "source": [
    "def dwt(x):\n",
    "  \"\"\"\n",
    "  Apply DWT and Highpass filter on the signal. \n",
    "  Raw signal is first read, low frequency signals are removed and denoising is \n",
    "  done by DWT.\n",
    "  Butterworth high pass digital filter is used to remove low frequency signals.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  x : numpy.ndarray\n",
    "    Raw signal.\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  x : numpy.ndarray\n",
    "    Denoised signal.\n",
    "\n",
    "  References\n",
    "  ----------\n",
    "  .. [1] https://www.kaggle.com/jackvial/dwt-signal-denoising\n",
    "  .. [2] http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "  .. [3] Threshold equation and using hard mode in threshold as mentioned\n",
    "         in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "         http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf  \n",
    "  .. [4] https://www.dropbox.com/s/2ltuvpw1b1ms2uu/A%20Complex%20Classification%20Approach%20of%20Partial%20Discharges%20from%20Covered%20Conductors%20in%20Real%20Environment%20%28preprint%29.pdf?dl=0\n",
    "  \"\"\"      \n",
    "  # Decompose to get the wavelet coefficients\n",
    "  # 'db4' wavelet is used because of the expert setting mentioned in the References [4]\n",
    "  # 'per' refers to periodic-padding, as the raw signal is a periodic one  \n",
    "  wavlt_coeffs = pywt.wavedec(x, 'db4', mode=\"per\")\n",
    "  \n",
    "  # Only the 1st level detailed coefficients are used to perform the thresholding and \n",
    "  # the 1s level approximate coefficients which contain low frequency compoenents (sine wave) are not used.\n",
    "  # Calculation of sigma value to perform threholding, is also as per References [3]\n",
    "  sigma = (1/0.6745) * np.mean(np.absolute(wavlt_coeffs[-1] - np.mean(wavlt_coeffs[-1],None)), None)\n",
    "  # Calculate the universal threshold using Stein Unbiased Risk Estimate (SURE), as per References [3]\n",
    "  Td = sigma * np.sqrt(2*np.log(len(x)))\n",
    "  # The calculated threshold value is used to perform 'hard-thresholding' on all the \n",
    "  # other detailed coefficient values.\n",
    "  wavlt_coeffs[1:] = (pywt.threshold(i, value=Td, mode='hard') for i in wavlt_coeffs[1:])\n",
    "\n",
    "  # Reconstruct the signal using the thresholded coefficients\n",
    "  x_dn = pywt.waverec(wavlt_coeffs, 'db4', mode='per')\n",
    "\n",
    "  # Remove signals whose frequency is below 10KHz (References [4]) as the PD pattern \n",
    "  # is found mostly above 10KHz\n",
    "  # Determine the cutoff frequency based on Nyquist criteria\n",
    "  sample_rate = len(x)/0.02\n",
    "  nyquist_rate = 0.5 * sample_rate\n",
    "  cut_off = 1e4/nyquist_rate\n",
    "\n",
    "  # Get the digital filter coefficients\n",
    "  # Order=10 referring to the 'strictness' of the cutoff\n",
    "  # Wn refers to the cutoff frequency value\n",
    "  # btype refers to the type of filter to be used\n",
    "  # output='sos' refers to second-order sections representations of IIR filter\n",
    "  filter_coeffs = butter(10, Wn=[cut_off], btype='highpass', output='sos')\n",
    "  # Apply the high pass filter on the DWT denoised signal\n",
    "  x_dn_hp = signal.sosfilt(filter_coeffs, x_dn)\n",
    "\n",
    "  return x_dn_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68H_9l_eXwOV"
   },
   "source": [
    "**Phase resolve features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "FS3g2lx_Xy4l"
   },
   "outputs": [],
   "source": [
    "def phase_resolve(args):\n",
    "  \"\"\"\n",
    "  Get the number of false, positive, and negative peaks present in each \n",
    "  location (I, II, III, IV quadrants) of the signal.\n",
    "\n",
    "  Following procedure is used:\n",
    "  *  Get the indices of true, false, symmetric, positive, negative peaks of the signal.\n",
    "  *  Get the phase angle of the raw signal.\n",
    "  *  Get the location of all the peaks for a 0 degree phase angled signal.\n",
    "  *  Add the phase angle of the signal to the location of the peak.\n",
    "  *  Aggregate the number of peaks in each quadrant.\n",
    "\n",
    "  Note:\n",
    "  Quadrants - \n",
    "  * I: 0 degree - 90 degree\n",
    "  * II: 90 degree - 180 degree\n",
    "  * III: 180 degree - 270 degree\n",
    "  * IV: 270 degree - 360 degree\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  args : Tuple\n",
    "    Tuple containing the following:\n",
    "    * sig_id\n",
    "    * sig_type - 'train' or 'test'\n",
    "    * phase_cols - names of the required features\n",
    "    * peak_val - minimum threshold value to consider a peak\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  phase_df : numpy.ndarray\n",
    "    Following features are returned:\n",
    "    * t_q0 - number of true peaks in I quadrant\n",
    "    * t_q1 - number of true peaks in II quadrant\n",
    "    * t_q2 - number of true peaks in III quadrant\n",
    "    * t_q3 - number of true peaks in IV quadrant\n",
    "    * f_q0 - number of false peaks in I quadrant\n",
    "    * f_q1 - number of false peaks in II quadrant\n",
    "    * f_q2 - number of false peaks in III quadrant\n",
    "    * f_q3 - number of false peaks in IV quadrant\n",
    "    * p_q0 - number of postive peaks in I quadrant\n",
    "    * p_q1 - number of postive peaks in II quadrant\n",
    "    * p_q2 - number of postive peaks in III quadrant\n",
    "    * p_q3 - number of postive peaks in IV quadrant\n",
    "    * n_q0 - number of negative peaks in I quadrant\n",
    "    * n_q1 - number of negative peaks in II quadrant\n",
    "    * n_q2 - number of negative peaks in III quadrant\n",
    "    * n_q3 - number of negative peaks in IV quadrant    \n",
    "  \"\"\"\n",
    "  sig_id = str(args[0])\n",
    "  sig_type = args[1][0]\n",
    "  phase_cols = args[1][1]\n",
    "  peak_val = args[1][2]\n",
    "\n",
    "  if sig_type == 'train':\n",
    "    x = pd.read_parquet(path + 'train.parquet', engine='fastparquet', columns=[sig_id])\n",
    "    x = x.values.flatten()\n",
    "  elif sig_type == 'test':\n",
    "    x = pd.read_parquet(path + 'test.parquet', engine='fastparquet', columns=[sig_id])\n",
    "    x = x.values.flatten()\n",
    "\n",
    "  x_dn = dwt(x)\n",
    "  t, f, s, p, n = get_all_peaks(x_dn, peak_val)\n",
    "\n",
    "  num_val = x.shape[0]\n",
    "  # Takes cosine wave as reference.\n",
    "  # 0 deg => 0 deg of cosine wave,\n",
    "  # 90 deg => 90 deg phase shift of cosine wave\n",
    "  omegas = np.exp(-2j * np.pi * np.arange(num_val) / num_val)\n",
    "  sig_phase = omegas.dot(x)\n",
    "  sig_phase_angle = np.angle(sig_phase, deg=False)\n",
    "\n",
    "  dt = (20e-3/(800000))\n",
    "  f1 = 50\n",
    "  w1 = 2*np.pi*f1\n",
    "\n",
    "  phase_df = pd.DataFrame(columns=phase_cols)\n",
    "  phases = [0, 1, 2, 3]\n",
    "  count = 0\n",
    "  for peak_inds in [t, f, p, n]:\n",
    "    peak_phase_angle_arr = []\n",
    "    for ind in peak_inds:\n",
    "      peak_phase_angle_arr.append((np.degrees((w1* ind *dt) + sig_phase_angle)) % 360)\n",
    "    \n",
    "    temp_df = pd.DataFrame(data=peak_phase_angle_arr, columns=['peak_arr'])\n",
    "    temp_df['quadrant'] = pd.cut(x=peak_phase_angle_arr,\n",
    "                                 bins=[0, 90, 180, 270, 360],\n",
    "                                 labels=phases)\n",
    "    phase_bins = temp_df['quadrant'].value_counts()\n",
    "    for phase in phases:\n",
    "      phase_df[phase_df.columns[count]] = phase_bins[phase_bins.index==phase].values\n",
    "      count += 1\n",
    "  \n",
    "  return phase_df.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1UjxlpuYBfu"
   },
   "source": [
    "**Fractal features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "CxO37WCp7T_l"
   },
   "outputs": [],
   "source": [
    "def get_fractal(fract_params):\n",
    "  \"\"\"\n",
    "  Calculate the given fractal features.\n",
    "  Following fractal features are calculated:\n",
    "  1. Petrosian fractal\n",
    "  2. Katz fractal\n",
    "  3. Detrended Fluctuation Analysis (DFA) fractal\n",
    "  4. Higuchi fractal\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  fract_params : list\n",
    "    List of function objects which are used to calculate the above \n",
    "    mentioned fractals.\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  temp_fd : list\n",
    "    List of above mentioned fractal values.\n",
    "\n",
    "  References\n",
    "  ----------\n",
    "  .. [1] https://hal.inria.fr/inria-00442374/document\n",
    "  .. [2] https://raphaelvallat.com/entropy/build/html/_modules/entropy/fractal.html#petrosian_fd\n",
    "  .. [3] http://tux.uis.edu.co/geofractales/articulosinteres/PDF/waveform.pdf  \n",
    "  .. [4] https://youtu.be/gB9n2gHsHN4\n",
    "  .. [5] https://www.youtube.com/watch?v=o0LndP2OlUI\n",
    "  .. [6] https://cschoel.github.io/nolds/nolds.html#detrended-fluctuation-analysis\n",
    "  .. [7] https://www.kaggle.com/tarunpaparaju/vsb-competition-attention-bilstm-with-features\n",
    "  .. [8] https://arxiv.org/ftp/arxiv/papers/0707/0707.1437.pdf\n",
    "  .. [9] https://www.sciencedirect.com/science/article/pii/S2212017313006506\n",
    "  .. [10] https://arxiv.org/pdf/cond-mat/0103510.pdf\n",
    "  \"\"\"\n",
    "  sig_id = str(fract_params[0])\n",
    "  sig_type = fract_params[1][0]\n",
    "  sig_type_1 = fract_params[1][1]\n",
    "  window_size = fract_params[1][2]\n",
    "  fract_fun = fract_params[1][3]\n",
    "\n",
    "  if sig_type_1 == 'test':\n",
    "      sig_data = pd.read_parquet(path + 'test.parquet', engine='fastparquet', columns=[sig_id])\n",
    "      sig_data = sig_data.values.flatten()\n",
    "  elif sig_type_1 == 'train':\n",
    "      sig_data = pd.read_parquet(path + 'train.parquet', engine='fastparquet', columns=[sig_id])\n",
    "      sig_data = sig_data.values.flatten()\n",
    "    \n",
    "  if sig_type == 'denoised':\n",
    "    sig_data = dwt(sig_data)\n",
    "  \n",
    "  no_winds = sig_data.shape[0]//window_size\n",
    "  rem_winds = sig_data.shape[0]%window_size\n",
    "  temp_fd = []\n",
    "\n",
    "  for func in fract_fun:\n",
    "    avg_fd = 0\n",
    "    for wind in range(no_winds):\n",
    "      avg_fd += func(sig_data[wind*window_size:(wind*window_size)+window_size])\n",
    "    if rem_winds != 0:\n",
    "      avg_fd += func(sig_data[(wind*window_size)+window_size:sig_data.shape[1]])\n",
    "    avg_fd /= no_winds\n",
    "    temp_fd.append(avg_fd)\n",
    "\n",
    "  return temp_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO3D14Dj4ZFJ"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "OMPDXZBbX-sz"
   },
   "outputs": [],
   "source": [
    "def get_entropy(entropy_params):\n",
    "  \"\"\"\n",
    "  Calculate the given entropy features:\n",
    "  Following entropy features are calculated:\n",
    "  1. Permutation entropy\n",
    "  2. SVD entropy\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  entropy_params : list\n",
    "    List of function objects which are used to calculate the above \n",
    "    mentioned entropy features.  \n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  temp_fd : list\n",
    "    List of above mentioned entropy values.\n",
    "\n",
    "  References\n",
    "  ----------\n",
    "  .. [1] https://www.mdpi.com/1099-4300/14/8/1553/htm\n",
    "  .. [2] https://core.ac.uk/download/pdf/25794249.pdf\n",
    "  .. [3] https://in.mathworks.com/help/signal/ref/pentropy.html#mw_a57f549d-996c-47d9-8d45-e80cb739ed41\n",
    "  .. [4] https://math.stackexchange.com/questions/542035/what-does-svd-entropy-capture\n",
    "  .. [5] https://raphaelvallat.com/entropy/build/html/generated/entropy.svd_entropy.html\n",
    "  .. [6] https://www.kaggle.com/tarunpaparaju/vsb-competition-attention-bilstm-with-features\n",
    "  .. [7] https://www.aptech.com/blog/permutation-entropy/\n",
    "  .. [8] https://www.hindawi.com/journals/complexity/2019/1403829/\n",
    "  .. [9] https://www.sciencedirect.com/science/article/abs/pii/S0888327011005140\n",
    "  .. [10] https://pdfs.semanticscholar.org/b769/8638ef9a9fa72c106f4c9829540c14070255.pdf?_ga=2.12379831.2082352284.1586606415-399075921.1580401470\n",
    "  \"\"\"\n",
    "  sig_id = str(entropy_params[0])\n",
    "  sig_type = entropy_params[1][0]\n",
    "  sig_type_1 = entropy_params[1][1]\n",
    "  window_size = entropy_params[1][2]\n",
    "  entropy_fun = entropy_params[1][3]\n",
    "  func_params = entropy_params[1][4]\n",
    "    \n",
    "  if sig_type_1 == 'train':\n",
    "    sig_data = pd.read_parquet(path + 'train.parquet', engine='fastparquet', columns=[sig_id])\n",
    "    sig_data = sig_data.values.flatten()\n",
    "  elif sig_type_1 == 'test':\n",
    "    sig_data = pd.read_parquet(path + 'test.parquet', engine='fastparquet', columns=[sig_id])\n",
    "    sig_data = sig_data.values.flatten()\n",
    "    \n",
    "  if sig_type == 'denoised':\n",
    "    sig_data = dwt(sig_data)\n",
    "\n",
    "  no_winds = sig_data.shape[0]//window_size\n",
    "  rem_winds = sig_data.shape[0]%window_size\n",
    "  \n",
    "\n",
    "  temp_ent = []\n",
    "\n",
    "  for func_ind, func in enumerate(entropy_fun):\n",
    "    avg_ent = 0\n",
    "    for wind in range(no_winds):\n",
    "      if func_ind not in func_params.keys():\n",
    "        avg_ent += func(sig_data[wind*window_size:(wind*window_size)+window_size])\n",
    "      else:\n",
    "        avg_ent += func(sig_data[wind*window_size:(wind*window_size)+window_size], int(func_params[func_ind]))\n",
    "    if rem_winds != 0:\n",
    "      if func_ind not in func_params.keys():\n",
    "        avg_fd += func(sig_data[(wind*window_size)+window_size:sig_data.shape[1]])\n",
    "      else:\n",
    "        avg_fd += func(sig_data[(wind*window_size)+window_size:sig_data.shape[1]], int(func_params[func_ind]))\n",
    "    avg_ent /= no_winds\n",
    "    temp_ent.append(avg_ent)\n",
    "\n",
    "  return temp_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QIcPb4Q-U0vj"
   },
   "source": [
    "### Fractal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "jFOCpNnMwgDs"
   },
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "SssTEzm-MKZi",
    "outputId": "ddd8b1ba-1d09-483d-e338-4cf9792e5ad3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing denoised signal\n",
      "Window: 5000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:28.406594\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:29.160470\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:28.379652\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:26.673831\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:26.743315\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:25.929440\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:26.460212\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:26.219586\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:26.157351\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:26.389630\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:17.185586\n",
      "Time taken for window size 5000: 0:34:48.200815\n",
      "\n",
      "Window: 10000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:31.655837\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:26.845285\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:26.972656\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:26.820671\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:26.647244\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:27.453236\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:27.182453\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:28.347670\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:28.053042\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:32.092483\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:17.356644\n",
      "Time taken for window size 10000: 0:34:59.911853\n",
      "\n",
      "Window: 50000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:35.271257\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:35.528636\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:35.306299\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:35.049083\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:36.968623\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:35.489715\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:39.305311\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:37.350522\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:37.565011\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:37.448200\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:18.224603\n",
      "Time taken for window size 50000: 0:36:23.991607\n",
      "\n",
      "Window: 100000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:43.106095\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:43.215122\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:43.591155\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:43.879984\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:43.981921\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:43.698146\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:43.500618\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:47.322194\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:44.157126\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:43.517834\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:18.642384\n",
      "Time taken for window size 100000: 0:37:39.095568\n",
      "\n",
      "Window: 800000\n",
      "864 (9.92%) signals completed. Time taken: 0:04:24.738328\n",
      "1728 (19.83%) signals completed. Time taken: 0:04:24.448975\n",
      "2592 (29.75%) signals completed. Time taken: 0:04:23.142462\n",
      "3456 (39.67%) signals completed. Time taken: 0:04:21.337203\n",
      "4320 (49.59%) signals completed. Time taken: 0:04:24.375680\n",
      "5184 (59.5%) signals completed. Time taken: 0:04:20.536899\n",
      "6048 (69.42%) signals completed. Time taken: 0:04:20.778353\n",
      "6912 (79.34%) signals completed. Time taken: 0:04:22.348340\n",
      "7776 (89.26%) signals completed. Time taken: 0:04:22.589319\n",
      "8640 (99.17%) signals completed. Time taken: 0:04:22.479330\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:21.814544\n",
      "Time taken for window size 800000: 0:44:09.067132\n",
      "\n",
      "Time taken for denoised signal: 3:08:00.268087\n",
      "\n",
      "\n",
      "Processing original signal\n",
      "Window: 5000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:20.034187\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:16.580809\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:16.605195\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:16.476250\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:16.419817\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:16.478801\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:16.130442\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:16.004832\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:19.831743\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:16.254252\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:16.358035\n",
      "Time taken for window size 5000: 0:33:07.666518\n",
      "\n",
      "Window: 10000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:18.155824\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:18.187011\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:18.324710\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:18.204600\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:18.549558\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:18.921540\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:22.235319\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:18.521669\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:18.086012\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:19.010163\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:16.502477\n",
      "Time taken for window size 10000: 0:33:25.189069\n",
      "\n",
      "Window: 50000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:25.563587\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:25.584419\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:25.526250\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:25.416571\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:27.943668\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:25.795107\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:25.515285\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:25.460484\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:25.438652\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:25.601219\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:17.068428\n",
      "Time taken for window size 50000: 0:34:35.400171\n",
      "\n",
      "Window: 100000\n",
      "864 (9.92%) signals completed. Time taken: 0:03:30.713858\n",
      "1728 (19.83%) signals completed. Time taken: 0:03:30.902596\n",
      "2592 (29.75%) signals completed. Time taken: 0:03:34.761788\n",
      "3456 (39.67%) signals completed. Time taken: 0:03:31.181366\n",
      "4320 (49.59%) signals completed. Time taken: 0:03:31.267467\n",
      "5184 (59.5%) signals completed. Time taken: 0:03:31.746930\n",
      "6048 (69.42%) signals completed. Time taken: 0:03:31.081135\n",
      "6912 (79.34%) signals completed. Time taken: 0:03:31.226026\n",
      "7776 (89.26%) signals completed. Time taken: 0:03:31.171321\n",
      "8640 (99.17%) signals completed. Time taken: 0:03:31.138844\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:17.606504\n",
      "Time taken for window size 100000: 0:35:33.288918\n",
      "\n",
      "Window: 800000\n",
      "864 (9.92%) signals completed. Time taken: 0:04:11.683688\n",
      "1728 (19.83%) signals completed. Time taken: 0:04:11.894235\n",
      "2592 (29.75%) signals completed. Time taken: 0:04:15.802042\n",
      "3456 (39.67%) signals completed. Time taken: 0:04:12.498849\n",
      "4320 (49.59%) signals completed. Time taken: 0:04:12.579985\n",
      "5184 (59.5%) signals completed. Time taken: 0:04:12.881267\n",
      "6048 (69.42%) signals completed. Time taken: 0:04:12.093509\n",
      "6912 (79.34%) signals completed. Time taken: 0:04:12.801746\n",
      "7776 (89.26%) signals completed. Time taken: 0:04:11.973498\n",
      "8640 (99.17%) signals completed. Time taken: 0:04:15.638170\n",
      "8712 (100.0%) signals completed. Time taken: 0:00:21.077082\n",
      "Time taken for window size 800000: 0:42:31.408191\n",
      "\n",
      "Time taken for original signal: 2:59:12.954095\n",
      "\n",
      "\n",
      "Total time taken: 6:07:13.223457\n"
     ]
    }
   ],
   "source": [
    "fractal_list = ['petrosian', 'katz', 'dfa', 'higuchi']\n",
    "fractal_funcs = [petrosian_fd, katz_fd, detrended_fluctuation, higuchi_fd]\n",
    "window_sizes = [5000, 10000, 50000, 100000, 800000]\n",
    "signal_type = ['denoised', 'original']\n",
    "\n",
    "start_total = datetime.now()\n",
    "for sig_type in signal_type:\n",
    "  print(f\"Processing {sig_type} signal\")\n",
    "  start_type = datetime.now()  \n",
    "  for wind in window_sizes:\n",
    "    print(f\"Window: {wind}\")\n",
    "    save_file_path = path + f'fractal_{wind}_{sig_type}_train.csv'\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    parallel_proc_func(get_fractal, fractal_list, metadata_train.shape[0], 0, save_file_path, (sig_type, 'train', wind, fractal_funcs))\n",
    "    \n",
    "    print(f\"Time taken for window size {wind}: {datetime.now()-start_time}\\n\")\n",
    "  print(f\"Time taken for {sig_type} signal: {datetime.now()-start_type}\\n\\n\")\n",
    "print(f\"Total time taken: {datetime.now()-start_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0NH52xHaZkBN"
   },
   "source": [
    "**Cluster features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     29,
     84
    ],
    "hidden": true,
    "id": "_jshrW74ZhLt"
   },
   "outputs": [],
   "source": [
    "def get_peak_index(func_params):\n",
    "  \"\"\"\n",
    "  Get the indices of all the 'true peaks' of the a signal.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  func_params : list\n",
    "    Contains the following parameters:\n",
    "    * signal id - func_params[0]\n",
    "      Id of the signal that is to be processed\n",
    "    * signal type - func_params[1]\n",
    "      Type (train or test) of signal to be processed.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  t : numpy.ndarray\n",
    "    True peak indices of the signal.\n",
    "  \"\"\"\n",
    "  sig_id = func_params[0]\n",
    "  sig_type = func_params[1]\n",
    "  if sig_type == 'train':\n",
    "    x = pd.read_parquet(path + 'train.parquet', engine='fastparquet', columns=[str(sig_id)])\n",
    "  else:\n",
    "    x = pd.read_parquet(path + 'test.parquet', engine='fastparquet', columns=[str(sig_id)])\n",
    "  x = dwt(x.values.flatten())\n",
    "  peak_thresh = np.load(path + 'best_peak_threshold.npy')\n",
    "  t, _, _, _, _ = get_all_peaks(x, int(peak_thresh))\n",
    "  return t\n",
    "\n",
    "def get_clusters(args):\n",
    "  \"\"\"\n",
    "  Assign a cluster value for all the true peak indices that are close to each other.\n",
    "  Two peak indices are considered to be from the same cluster if the distance\n",
    "  between them is less than the given threshold.\n",
    "\n",
    "  Clustering process example:\n",
    "  Let t_ind_1 and t_ind_2 be two true peak index values of a signal.\n",
    "  Let perc be the minimum difference needed between consecutive indices to be\n",
    "  part of the same cluster.\n",
    "  Step 1: Assign a value to t_ind_1, say 0 i.e., t_ind_1_cluster = 0\n",
    "  Step 2: Get the difference between t_ind_1 and t_ind_2 i.e., \n",
    "          t_ind_diff = (t_ind_2 - t_ind_1)\n",
    "  Step 3: If difference is less than threshold, then make t_ind_2 part of the \n",
    "          same cluster as t_ind_1, i.e., t_ind_2_cluster = 0\n",
    "          If difference is greater than threshold, then make t_ind_2 part of a\n",
    "          differenct cluster as t_ind_1, i.e., t_ind_2_cluster = 1\n",
    "  Repeat the process for all the indices.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  args : list\n",
    "    Following parameters are passed:\n",
    "    * signal id : string\n",
    "      Id of the signal whose clustering has to be done\n",
    "    * perc : int\n",
    "      The threshold value which gives the minimum distance the consecutive peak\n",
    "      indices should have to be considered as part of the same cluster.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  cluster_arr : numpy.ndarray\n",
    "    Array containing the cluster values for each true index.\n",
    "  \"\"\"\n",
    "  sig_id = int(args[0])\n",
    "  perc = args[1]  \n",
    "\n",
    "  peak_inds = peak_cluster_df.iloc[sig_id].dropna()\n",
    "  num_peak_inds = len(peak_inds)\n",
    "  if num_peak_inds <= 1:\n",
    "    cluster_arr = np.zeros((1))\n",
    "    return cluster_arr\n",
    "\n",
    "  cluster_arr = np.zeros((num_peak_inds))\n",
    "  peak_inds_diff = np.diff(peak_inds, n=1)\n",
    "  thresh = np.percentile(peak_inds_diff, perc)\n",
    "  cluster = 0\n",
    "  cluster_arr[0] = cluster\n",
    "\n",
    "  for ind, val in enumerate(peak_inds_diff):\n",
    "    if val > thresh:\n",
    "      cluster += 1  \n",
    "    cluster_arr[ind+1] = cluster  \n",
    "  return cluster_arr\n",
    "\n",
    "def get_cluster_groups(args):\n",
    "  \"\"\"\n",
    "  Get the number of true peak indices present in each clusters of the signal.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  args : list\n",
    "    Following parameters are passed:\n",
    "    * signal id : string\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "    List containing following parameters are returned:\n",
    "    * total_clusters : int\n",
    "      Total number of clusters present in the signal\n",
    "    * num_cluster_groups : int\n",
    "      Number of clusters having more than 2 indices.\n",
    "  \"\"\"\n",
    "  sig_id = int(args)\n",
    "  clean_clusters = peak_cluster_group.iloc[sig_id].dropna()\n",
    "  total_cluster_groups = clean_clusters.value_counts()\n",
    "\n",
    "  total_clusters = clean_clusters[-1]\n",
    "  if total_cluster_groups[0] <= 2:\n",
    "    num_cluster_groups = 0\n",
    "  else:\n",
    "    num_cluster_groups = len([x for x in total_cluster_groups if x > 2])\n",
    "  return [total_clusters, num_cluster_groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "RRdMvGETGZx-"
   },
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "KdEUVSMCGZx-",
    "outputId": "bc3207d3-bfbe-47be-99e5-cba208fc1566",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing denoised signal\n",
      "Window: 5000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:52.448453\n",
      "4064 (19.98%) signals completed. Time taken: 0:27:46.979983\n",
      "6096 (29.97%) signals completed. Time taken: 0:27:49.889123\n",
      "8128 (39.97%) signals completed. Time taken: 0:27:46.505425\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:46.364512\n",
      "12192 (59.95%) signals completed. Time taken: 0:27:50.133575\n",
      "14224 (69.94%) signals completed. Time taken: 0:27:47.068105\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:48.599413\n",
      "18288 (89.92%) signals completed. Time taken: 0:27:51.819849\n",
      "20320 (99.92%) signals completed. Time taken: 0:27:54.136744\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.366915\n",
      "Time taken for window size 5000: 4:38:32.420754\n",
      "\n",
      "Window: 10000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:52.832234\n",
      "4064 (19.98%) signals completed. Time taken: 0:27:54.669654\n",
      "6096 (29.97%) signals completed. Time taken: 0:27:55.572894\n",
      "8128 (39.97%) signals completed. Time taken: 0:27:53.644396\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:53.211076\n",
      "12192 (59.95%) signals completed. Time taken: 0:27:52.346026\n",
      "14224 (69.94%) signals completed. Time taken: 0:27:54.667813\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:56.927506\n",
      "18288 (89.92%) signals completed. Time taken: 0:28:01.304944\n",
      "20320 (99.92%) signals completed. Time taken: 0:28:02.739617\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.906361\n",
      "Time taken for window size 10000: 4:39:36.914771\n",
      "\n",
      "Window: 50000\n",
      "2032 (9.99%) signals completed. Time taken: 0:28:19.530482\n",
      "4064 (19.98%) signals completed. Time taken: 0:28:16.587313\n",
      "6096 (29.97%) signals completed. Time taken: 0:28:14.978643\n",
      "8128 (39.97%) signals completed. Time taken: 0:28:14.358728\n",
      "10160 (49.96%) signals completed. Time taken: 0:28:21.895377\n",
      "12192 (59.95%) signals completed. Time taken: 0:28:25.181564\n",
      "14224 (69.94%) signals completed. Time taken: 0:28:28.848417\n",
      "16256 (79.93%) signals completed. Time taken: 0:28:31.108740\n",
      "18288 (89.92%) signals completed. Time taken: 0:28:30.161070\n",
      "20320 (99.92%) signals completed. Time taken: 0:28:29.081723\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.873156\n",
      "Time taken for window size 50000: 4:44:10.690159\n",
      "\n",
      "Window: 100000\n",
      "2032 (9.99%) signals completed. Time taken: 0:28:44.941814\n",
      "4064 (19.98%) signals completed. Time taken: 0:28:43.227092\n",
      "6096 (29.97%) signals completed. Time taken: 0:28:34.923887\n",
      "8128 (39.97%) signals completed. Time taken: 0:28:33.197081\n",
      "10160 (49.96%) signals completed. Time taken: 0:28:38.258665\n",
      "12192 (59.95%) signals completed. Time taken: 0:28:36.333954\n",
      "14224 (69.94%) signals completed. Time taken: 0:28:35.280039\n",
      "16256 (79.93%) signals completed. Time taken: 0:28:27.805808\n",
      "18288 (89.92%) signals completed. Time taken: 0:28:30.660723\n",
      "20320 (99.92%) signals completed. Time taken: 0:28:28.827522\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.719478\n",
      "Time taken for window size 100000: 4:46:12.261292\n",
      "\n",
      "Window: 800000\n",
      "2032 (9.99%) signals completed. Time taken: 0:30:18.411498\n",
      "4064 (19.98%) signals completed. Time taken: 0:30:11.197616\n",
      "6096 (29.97%) signals completed. Time taken: 0:30:17.634198\n",
      "8128 (39.97%) signals completed. Time taken: 0:30:12.055206\n",
      "10160 (49.96%) signals completed. Time taken: 0:30:12.134762\n",
      "12192 (59.95%) signals completed. Time taken: 0:30:13.280781\n",
      "14224 (69.94%) signals completed. Time taken: 0:30:13.225214\n",
      "16256 (79.93%) signals completed. Time taken: 0:30:20.799734\n",
      "18288 (89.92%) signals completed. Time taken: 0:30:12.118759\n",
      "20320 (99.92%) signals completed. Time taken: 0:30:09.711880\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:18.819239\n",
      "Time taken for window size 800000: 5:02:40.483898\n",
      "\n",
      "Time taken for denoised signal: 23:51:12.771804\n",
      "\n",
      "\n",
      "Processing original signal\n",
      "Window: 5000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:18.164681\n",
      "4064 (19.98%) signals completed. Time taken: 0:27:18.260344\n",
      "6096 (29.97%) signals completed. Time taken: 0:27:20.116060\n",
      "8128 (39.97%) signals completed. Time taken: 0:27:20.830331\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:22.120539\n",
      "12192 (59.95%) signals completed. Time taken: 0:27:21.647489\n",
      "14224 (69.94%) signals completed. Time taken: 0:27:23.309324\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:34.054654\n",
      "18288 (89.92%) signals completed. Time taken: 0:27:34.217935\n",
      "20320 (99.92%) signals completed. Time taken: 0:27:25.068881\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.137373\n",
      "Time taken for window size 5000: 4:34:16.015637\n",
      "\n",
      "Window: 10000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:28.390698\n",
      "4064 (19.98%) signals completed. Time taken: 0:27:30.202141\n",
      "6096 (29.97%) signals completed. Time taken: 0:27:28.938725\n",
      "8128 (39.97%) signals completed. Time taken: 0:27:28.141420\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:27.128014\n",
      "12192 (59.95%) signals completed. Time taken: 0:27:25.891261\n",
      "14224 (69.94%) signals completed. Time taken: 0:27:31.526727\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:29.698252\n",
      "18288 (89.92%) signals completed. Time taken: 0:27:33.659519\n",
      "20320 (99.92%) signals completed. Time taken: 0:27:32.508732\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.112012\n",
      "Time taken for window size 10000: 4:35:14.279671\n",
      "\n",
      "Window: 50000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:50.723907\n",
      "4064 (19.98%) signals completed. Time taken: 0:27:48.818299\n",
      "6096 (29.97%) signals completed. Time taken: 0:27:50.835390\n",
      "8128 (39.97%) signals completed. Time taken: 0:27:51.932366\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:51.691280\n",
      "12192 (59.95%) signals completed. Time taken: 0:27:55.212664\n",
      "14224 (69.94%) signals completed. Time taken: 0:27:58.491454\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:54.131537\n",
      "18288 (89.92%) signals completed. Time taken: 0:27:49.519791\n",
      "20320 (99.92%) signals completed. Time taken: 0:27:49.717724\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.231580\n",
      "Time taken for window size 50000: 4:38:59.384758\n",
      "\n",
      "Window: 100000\n",
      "2032 (9.99%) signals completed. Time taken: 0:27:58.203545\n",
      "4064 (19.98%) signals completed. Time taken: 0:28:04.368098\n",
      "6096 (29.97%) signals completed. Time taken: 0:28:00.191892\n",
      "8128 (39.97%) signals completed. Time taken: 0:28:00.380548\n",
      "10160 (49.96%) signals completed. Time taken: 0:27:59.220696\n",
      "12192 (59.95%) signals completed. Time taken: 0:28:03.096542\n",
      "14224 (69.94%) signals completed. Time taken: 0:28:00.368667\n",
      "16256 (79.93%) signals completed. Time taken: 0:27:57.637935\n",
      "18288 (89.92%) signals completed. Time taken: 0:28:00.301256\n",
      "20320 (99.92%) signals completed. Time taken: 0:28:01.942813\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:17.561512\n",
      "Time taken for window size 100000: 4:40:24.358682\n",
      "\n",
      "Window: 800000\n",
      "2032 (9.99%) signals completed. Time taken: 0:29:57.702785\n",
      "4064 (19.98%) signals completed. Time taken: 0:29:56.040353\n",
      "6096 (29.97%) signals completed. Time taken: 0:29:58.975444\n",
      "8128 (39.97%) signals completed. Time taken: 0:29:56.889178\n",
      "10160 (49.96%) signals completed. Time taken: 0:29:50.836854\n",
      "12192 (59.95%) signals completed. Time taken: 0:29:53.497659\n",
      "14224 (69.94%) signals completed. Time taken: 0:29:51.021310\n",
      "16256 (79.93%) signals completed. Time taken: 0:29:51.166573\n",
      "18288 (89.92%) signals completed. Time taken: 0:29:53.037140\n",
      "20320 (99.92%) signals completed. Time taken: 0:29:55.319683\n",
      "20337 (100.0%) signals completed. Time taken: 0:00:18.610620\n",
      "Time taken for window size 800000: 4:59:24.184518\n",
      "\n",
      "Time taken for original signal: 23:28:18.224063\n",
      "\n",
      "\n",
      "Total time taken: 1 day, 23:19:30.996711\n"
     ]
    }
   ],
   "source": [
    "fractal_list = ['petrosian', 'katz', 'dfa', 'higuchi']\n",
    "fractal_funcs = [petrosian_fd, katz_fd, detrended_fluctuation, higuchi_fd]\n",
    "window_sizes = [5000, 10000, 50000, 100000, 800000]\n",
    "signal_type = ['denoised', 'original']\n",
    "\n",
    "start_total = datetime.now()\n",
    "for sig_type in signal_type:\n",
    "  print(f\"Processing {sig_type} signal\")\n",
    "  start_type = datetime.now()  \n",
    "  for wind in window_sizes:\n",
    "    print(f\"Window: {wind}\")\n",
    "    save_file_path = path + f'test_path/fractal_{wind}_{sig_type}_test.csv'\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    parallel_proc_func(get_fractal, fractal_list, metadata_test.shape[0], metadata_test['signal_id'].loc[0],\n",
    "                       save_file_path, (sig_type, 'test', wind, fractal_funcs))\n",
    "    \n",
    "    print(f\"Time taken for window size {wind}: {datetime.now()-start_time}\\n\")\n",
    "  print(f\"Time taken for {sig_type} signal: {datetime.now()-start_type}\\n\\n\")\n",
    "print(f\"Total time taken: {datetime.now()-start_total}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1ZTBF18ZEONo",
    "JRnqVf-QDw94",
    "XgcVEnO5J2Pd",
    "gtM51xaipBOP",
    "LjyJVeSgK7IP",
    "EV0E8zajp97B",
    "HUcQH4adkpYl",
    "gToBc-Nbqa9a",
    "0KkHu4XtqoFB",
    "uxMx8jS79sX5",
    "LsJZ8qjv0vI1",
    "yM1hodjzsoIX",
    "tasQHLPFHx0K",
    "5ljTuuyvrNIL",
    "gRQgQzM1w4-T",
    "hYnDAggwulEs",
    "OO3D14Dj4ZFJ",
    "fYOVUWM2CgmB",
    "Fr-IvNCcCgmC",
    "xZnqZYomCgmC",
    "oARDCYrcCgmC",
    "VuIso9dZCgmD",
    "Dz5yR5izCgmD",
    "Xyx6O_oWCgmD",
    "xnEmDQAMCgmD",
    "0LeCSMSGCgmD",
    "WcPOFlxSCgmD",
    "-jmumq9wCgmD",
    "TbK29nrdCgmD",
    "RzxqTnY1CgmE",
    "FPb1daCTEtvR",
    "zF5eTGRKNMmr",
    "_GdAy-bXGRl0",
    "msbyL45DNOXr",
    "dv4OWN88Ngb1",
    "XGi-VaePOt1k",
    "keRv86CZO9xq",
    "X8odEUTSPK9s",
    "C98Gw4ARQoTO",
    "JHyirFAjQ2lb",
    "C2aRgctdRKEX",
    "q5zfAtexRWv1",
    "ElFR5VfQBUSF",
    "j8QuP3GPBi1X",
    "7BM6KBuFrJce",
    "sqXJm-79CAeQ",
    "jbf2IyOF80da",
    "btcU43941Puq",
    "gupdFDvPrOwC",
    "d2h1FH1m5NkB",
    "KK25ZIPR5wS9",
    "2RwoZ5rouiEB",
    "Qla-BFM7vJXo",
    "drVA7ZZw-G1k",
    "xc6T3IQw9XMU",
    "KcgtUXbiEGvs",
    "l3SNZreRGZx-",
    "DnjaaUbp-gE6",
    "EhEZqJHIGZx-",
    "4EuO2uon0VMy",
    "7jLADcBJ1dnB",
    "2jCnVDVm1iIP",
    "pcrBS0OW1sos",
    "5s4ny3pXGZx-",
    "QIcPb4Q-U0vj",
    "jFOCpNnMwgDs",
    "xHDKB9Gp5MHT",
    "a810Vb5pU0vk",
    "gbTiVhIKwjUl",
    "SA9_vkih37Uz",
    "PjywlBnF39lj",
    "5y6uUp262vbS",
    "7xFB09VZ4B0o",
    "llLT_00L4DrT",
    "aHh6Xthg3R45",
    "ZqVx8gaY4Tar",
    "KByXWV8Y4aru",
    "vUHZXCq92-35",
    "x6BXLaFg4jyK",
    "umjlmIXT4l7G",
    "RRdMvGETGZx-",
    "UbhOZDQdV8Tm",
    "8TJiCot24__j",
    "S-uBHyt06DRC",
    "5MD85_TXY54d",
    "QgW5AqDTZJpA",
    "hSPRnnl5ZJpB",
    "WQJqjp_kZJpF",
    "dSfdGq5VZJpQ",
    "43-LvskSZJpQ",
    "82pDRK8CZJpT",
    "IKdB79l5GZx_",
    "LEpjtL5X62PM",
    "EaPpYiD2zJIM",
    "xFYDWi-c9QSV",
    "gV6IDO5vMyyX",
    "DwutsCUSM2en",
    "4Zb5UwGDGZx_",
    "EY12aYie-TmH",
    "QF67LVOc17v4",
    "qMS1Er9I19zR",
    "im-2Zub06GiZ",
    "yzAThKvLorI3",
    "6fpOK5bocelf",
    "sYRhga4ZncZM",
    "kdxPCrzYd7XN",
    "xZvPkqlw63Kq",
    "4GbL-LPL66aq",
    "77mRapoM-90w",
    "MC2EsEvd6w-y",
    "Zaan_vY56z4g",
    "a6cpJ-NR5N_F",
    "arS7UEjz6rH0",
    "3oWMWNbC6led",
    "M6rpI29mLTGy",
    "QqKqanlTnG6j",
    "QMuwO6tYdUIh",
    "Lj8-wobFnsCq",
    "GTkKBVdWnsCs",
    "_0496LywnsDJ",
    "OxkWAD73nsDL",
    "SCsQjhIfnsDk",
    "UOycTgLxnsD8",
    "npI5lMQNnsD8",
    "nfqhJLb3nsEF",
    "fvzwfNcnnsEN",
    "snY_NxoJnsEN",
    "4cPEeu2ZnsET",
    "KvXcRixjnsEZ",
    "FUfYt1N9nsEZ",
    "D3TMq62QnsEk",
    "sDX8SnB1q7nl",
    "hTltbNwvq7nl",
    "faVJKdw9zd3S",
    "5E2nweRqwiV7",
    "0cHAHKTxlelz",
    "X2ek805sdM9v",
    "2cuigi9-OFMG",
    "QLbbLNOWq7np",
    "1XO6BJLWPJL7",
    "sYlvie8o1D8H",
    "eVk7-kOO58h-",
    "LK9suT9z5-6l",
    "lQ4Fl6aa6But",
    "TXhM5Etk7yVL",
    "ctqQ_Hnc7yVL",
    "pHP1LyGY7yVM",
    "h_Ed7rzW7yVN",
    "3EolLNL37yVN",
    "L8j0zBO--1or",
    "tJhr_h2m-1or",
    "NNU3PwL3-1ot",
    "9CsulyWn-1ot",
    "H53uRi9sFZ2v",
    "KwfR8GtOFZ2w",
    "U2LOgxPUFZ2z",
    "7jzYJ4LOFZ20",
    "5CGR08PLFZ21",
    "yxD65yj6HOg4",
    "G7BPSsJ-HOg5",
    "vhYW5o8KHOg7",
    "ryqigjWKHOg7",
    "_1EnfeVZHOg8",
    "CcRofhU6AzDI",
    "B1R4Ewp4AzDJ",
    "wIFcY9NZAzDM",
    "rsMf1jq6AzDN",
    "N8l2GJpcAzDO",
    "TqD2ThLyCIxr",
    "m_QtW0TKCIxr",
    "HQSDSXDPCIxs",
    "ZR2efNgXCIxs",
    "eUow40PeCIxt",
    "6EYKo_ALEUWZ",
    "XAenXHLAEUWa",
    "HYe1v0u7EUWa",
    "kv3j7abdIOhO",
    "3gLM0UIVIOhP",
    "7Am_0YlDIOhS",
    "vs97CsTRIOhS",
    "JFSjWiftIOhT",
    "e3ULDYVfJPME",
    "MTKJLsBDJPMF",
    "nslKt_g5JPMJ",
    "3QNzWWf5JPMK",
    "VS9S0tiDJPMK",
    "OQvRh1JPKBIP",
    "dLdM0ME6KBIS",
    "pu97JjrDKBIS",
    "WBjuD9whKBIS",
    "PVNcNmveKqyZ",
    "O4_0fYcWK2b1",
    "LAIuTrJGK2b2",
    "jg8zM4lpKqyb",
    "psOw6rZkKqyb",
    "qkWprEitMW_x",
    "RXNYBI80MW_x",
    "LZeePJsBMW_y",
    "LxDW542aMW_y",
    "VPn1PontMW_y",
    "sy0S_uccMucU",
    "alzIrautMucU",
    "SAtpWOUsMucV",
    "O33TFoFBMucV",
    "a6w2xZQaMucV",
    "LIzfsoGySllw"
   ],
   "machine_shape": "hm",
   "name": "29_VSB_Power_Line_featurization_part_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
